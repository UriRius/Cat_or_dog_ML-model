{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b30bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f011397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a extret: ../data/images\n",
      "a extret: ../data/annotations\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tar_gz(file_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        print(f\"Extraient {file_path}...\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=extract_to)\n",
    "        print(f\"Extret a: {extract_to}\")\n",
    "    else:\n",
    "        print(f\"a extret: {extract_to}\")\n",
    "\n",
    "# Descomprimir\n",
    "extract_tar_gz(\"../data/images.tar.gz\", \"../data/images\")\n",
    "extract_tar_gz(\"../data/annotations.tar.gz\", \"../data/annotations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd46452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imatges reorganitzades correctament.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainval_file = \"../data/annotations/annotations/trainval.txt\"\n",
    "images_dir = \"../data/images/images\"\n",
    "\n",
    "with open(trainval_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "train_lines, val_lines = train_test_split(lines, test_size=0.2, random_state=42)\n",
    "\n",
    "def reorganize(lines, split=\"train\"):\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        image_name = parts[0] + \".jpg\"\n",
    "        species = int(parts[2])  # 1 = cat, 2 = dog\n",
    "        \n",
    "        label = \"cats\" if species == 1 else \"dogs\"\n",
    "\n",
    "        src_path = os.path.normpath(os.path.join(images_dir, image_name))\n",
    "        dst_dir = os.path.join(\"../data\", split, label)\n",
    "        dst_path = os.path.join(dst_dir, image_name)\n",
    "\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "# Reorganitzar imatges\n",
    "reorganize(train_lines, split=\"train\")\n",
    "reorganize(val_lines, split=\"val\")\n",
    "print(\"Imatges reorganitzades correctament.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19aeeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformació bàsica\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Datasets i DataLoaders\n",
    "dataset_train = ImageFolder(\"../data/train\", transform=transform)\n",
    "dataset_val = ImageFolder(\"../data/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33645ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SoftMax(nn.Module):\n",
    "    def __init__(self, input_size=128*128*3, num_classes=2):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e06719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)  # 128x128 input, 2 pools (halven la mida 128 -> 64 -> 32)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # batch x 16 x 64 x 64\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # batch x 32 x 32 x 32\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272876db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax(\n",
      "  (linear): Linear(in_features=49152, out_features=2, bias=True)\n",
      ")\n",
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=32768, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#imatges 128x128 RGB\n",
    "softmax_model = SoftMax(input_size=128*128*3, num_classes=2)\n",
    "\n",
    "simplecnn_model = SimpleCNN(num_classes=2)\n",
    "\n",
    "print(softmax_model)\n",
    "print(simplecnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62427132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_softmax = optim.SGD(softmax_model.parameters(), lr=0.1, momentum=0.1)\n",
    "optimizer_simplecnn = optim.SGD(simplecnn_model.parameters(), lr=0.1, momentum=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7deaa94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f301aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n",
      "Iter 1, Loss: 0.6843\n",
      "Iter 31, Loss: 22.8428\n",
      "Iter 61, Loss: 41.5551\n",
      "Iter 91, Loss: 61.1551\n",
      "Iter 121, Loss: 79.3222\n",
      "Iter 151, Loss: 97.5567\n",
      "Iter 181, Loss: 118.5495\n",
      "Iter 211, Loss: 137.8146\n",
      "Iter 241, Loss: 155.1523\n",
      "Iter 271, Loss: 174.0639\n",
      "Iter 301, Loss: 192.4060\n",
      "Iter 331, Loss: 211.7655\n",
      "Iter 361, Loss: 230.2677\n",
      "Iter 391, Loss: 249.9545\n",
      "Iter 421, Loss: 269.7781\n",
      "Iter 451, Loss: 288.3966\n",
      "Iter 481, Loss: 305.2344\n",
      "Iter 511, Loss: 323.5838\n",
      "Iter 541, Loss: 340.1402\n",
      "Iter 571, Loss: 357.4550\n",
      "Epoch 1, Loss: 368.7257, Accuracy: 0.6576\n",
      "Iter 601, Loss: 6.7772\n",
      "Iter 631, Loss: 23.8060\n",
      "Iter 661, Loss: 41.0741\n",
      "Iter 691, Loss: 57.0322\n",
      "Iter 721, Loss: 73.7836\n",
      "Iter 751, Loss: 90.8226\n",
      "Iter 781, Loss: 108.4393\n",
      "Iter 811, Loss: 124.1039\n",
      "Iter 841, Loss: 142.7686\n",
      "Iter 871, Loss: 158.8139\n",
      "Iter 901, Loss: 174.3726\n",
      "Iter 931, Loss: 191.8272\n",
      "Iter 961, Loss: 209.0023\n",
      "Iter 991, Loss: 226.4794\n",
      "Iter 1021, Loss: 242.5197\n",
      "Iter 1051, Loss: 258.9680\n",
      "Iter 1081, Loss: 276.0611\n",
      "Iter 1111, Loss: 293.3380\n",
      "Iter 1141, Loss: 307.8759\n",
      "Iter 1171, Loss: 323.8864\n",
      "Epoch 2, Loss: 328.1170, Accuracy: 0.6929\n",
      "Iter 1201, Loss: 10.9993\n",
      "Iter 1231, Loss: 25.1223\n",
      "Iter 1261, Loss: 38.5016\n",
      "Iter 1291, Loss: 54.5899\n",
      "Iter 1321, Loss: 67.3996\n",
      "Iter 1351, Loss: 81.9339\n",
      "Iter 1381, Loss: 99.1557\n",
      "Iter 1411, Loss: 111.8371\n",
      "Iter 1441, Loss: 123.7093\n",
      "Iter 1471, Loss: 140.2981\n",
      "Iter 1501, Loss: 153.9445\n",
      "Iter 1531, Loss: 168.5084\n",
      "Iter 1561, Loss: 182.7858\n",
      "Iter 1591, Loss: 197.0513\n",
      "Iter 1621, Loss: 209.5015\n",
      "Iter 1651, Loss: 222.5940\n",
      "Iter 1681, Loss: 235.0651\n",
      "Iter 1711, Loss: 249.7440\n",
      "Iter 1741, Loss: 265.8468\n",
      "Epoch 3, Loss: 279.5005, Accuracy: 0.7038\n",
      "Iter 1771, Loss: 1.6341\n",
      "Iter 1801, Loss: 11.6775\n",
      "Iter 1831, Loss: 20.2174\n",
      "Iter 1861, Loss: 30.4016\n",
      "Iter 1891, Loss: 38.6050\n",
      "Iter 1921, Loss: 46.8128\n",
      "Iter 1951, Loss: 58.7283\n",
      "Iter 1981, Loss: 68.6217\n",
      "Iter 2011, Loss: 80.0623\n",
      "Iter 2041, Loss: 89.8065\n",
      "Iter 2071, Loss: 99.7652\n",
      "Iter 2101, Loss: 110.3590\n",
      "Iter 2131, Loss: 121.7050\n",
      "Iter 2161, Loss: 131.4220\n",
      "Iter 2191, Loss: 145.0545\n",
      "Iter 2221, Loss: 155.6414\n",
      "Iter 2251, Loss: 163.8450\n",
      "Iter 2281, Loss: 173.0117\n",
      "Iter 2311, Loss: 181.7459\n",
      "Iter 2341, Loss: 192.9230\n",
      "Epoch 4, Loss: 198.2993, Accuracy: 0.7024\n",
      "Iter 2371, Loss: 2.7006\n",
      "Iter 2401, Loss: 8.6659\n",
      "Iter 2431, Loss: 14.1086\n",
      "Iter 2461, Loss: 17.7391\n",
      "Iter 2491, Loss: 20.9645\n",
      "Iter 2521, Loss: 24.2783\n",
      "Iter 2551, Loss: 28.1306\n",
      "Iter 2581, Loss: 32.7638\n",
      "Iter 2611, Loss: 36.2690\n",
      "Iter 2641, Loss: 42.4019\n",
      "Iter 2671, Loss: 47.3340\n",
      "Iter 2701, Loss: 52.3860\n",
      "Iter 2731, Loss: 57.8157\n",
      "Iter 2761, Loss: 62.7274\n",
      "Iter 2791, Loss: 68.3161\n",
      "Iter 2821, Loss: 73.7645\n",
      "Iter 2851, Loss: 77.4306\n",
      "Iter 2881, Loss: 80.9283\n",
      "Iter 2911, Loss: 84.3689\n",
      "Iter 2941, Loss: 89.3509\n",
      "Epoch 5, Loss: 89.5687, Accuracy: 0.6916\n",
      "Maximum validation accuracy: 0.7038043478260869\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "n_epochs = 5\n",
    "model = simplecnn_model\n",
    "optimizer = optimizer_simplecnn\n",
    "iteracions = 0\n",
    "print(len(train_loader))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if iteracions % 250 == 0:\n",
    "            print(f\"Iter {iteracions+1}, Loss: {running_loss:.4f}\")\n",
    "        iteracions+=1\n",
    "\n",
    "    # Validació\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "\n",
    "print(\"Maximum validation accuracy:\", max_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d61bd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def predict_image(image_path, model, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    classes = ['cat', 'dog']  \n",
    "    return classes[predicted.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4002940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_shorthaired_115.jpg → Predicció: dog\n",
      "Bengal_136.jpg → Predicció: dog\n",
      "Birman_165.jpg → Predicció: cat\n",
      "english_setter_160.jpg → Predicció: dog\n",
      "basset_hound_170.jpg → Predicció: dog\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "idx_to_class = {0: \"cat\", 1: \"dog\"}\n",
    "\n",
    "val_dir = \"../data/val\"\n",
    "all_images = []\n",
    "\n",
    "for label_dir in os.listdir(val_dir):\n",
    "    full_path = os.path.join(val_dir, label_dir)\n",
    "    if os.path.isdir(full_path):\n",
    "        for fname in os.listdir(full_path):\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                all_images.append(os.path.join(full_path, fname))\n",
    "\n",
    "sample_images = random.sample(all_images, 5)\n",
    "\n",
    "model.eval()\n",
    "for img_path in sample_images:\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "        label = idx_to_class[pred]\n",
    "\n",
    "    print(f\"{os.path.basename(img_path)} → Predicció: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1713ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def predict_image(img_path, model, transform, class_names=[\"cats\", \"dogs\"]):\n",
    "    image = ImageOps.exif_transpose(Image.open(img_path)).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0) \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return class_names[predicted.item()], image\n",
    "\n",
    "directory_in_str = \"../data/val/proves\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "image_files = [f for f in os.listdir(directory_in_str) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "\n",
    "for filename in image_files:\n",
    "    img_path = os.path.join(directory_in_str, filename)\n",
    "    prediction, image = predict_image(img_path, model, transform)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Prediction: {prediction}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
